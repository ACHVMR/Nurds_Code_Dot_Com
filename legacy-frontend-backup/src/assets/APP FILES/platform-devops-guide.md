# ğŸš€ AchieveMore Platform: Complete End-to-End Implementation Guide
## Central Development Machine Architecture for 1M+ Concurrent Users

**Document Version:** 2.0  
**Prepared For:** Engineering & DevOps Teams  
**Date:** January 5, 2026  
**Classification:** Internal - Engineering Use  
**Estimated Reading Time:** 90 minutes

---

## TABLE OF CONTENTS

1. **Platform Architecture Overview**
2. **Central Development Machine (CDM) Design**
3. **The Oracle Framework: Efficient Code Generation System**
4. **Terminal Instructions & CLI Toolkit**
5. **End-to-End Request Processing Pipeline**
6. **Multi-Region Sharding & Scalability**
7. **Temporal Workflow Orchestration**
8. **Resource Management & Cost Optimization**
9. **Security & Isolation Layer**
10. **Monitoring, Logging & Observability**
11. **Disaster Recovery & High Availability**
12. **Production Deployment Checklist**

---

## 1. PLATFORM ARCHITECTURE OVERVIEW

### 1.1 System Design Philosophy

Unlike traditional SaaS platforms that build one product for everyone, AchieveMore operates as a **meta-platform**: a central infrastructure that spawns 100+ specialized applications (Spokes), each serving different markets.

This architecture is inspired by:
- **Replit** - Cloud-based development environment with containerized execution
- **Cursor IDE** - AI-powered code generation with codebase context
- **VibeCode Framework** - Enterprise vibe coding with governance guardrails
- **Temporal.io** - Durable execution for reliable agent orchestration

**Key Design Tenets:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AchieveMore Platform Architecture                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Multi-Region Load Balancer (Global Entry Point) â”‚  â”‚
â”‚  â”‚  US-EAST | EU-WEST | ASIA-PACIFIC | CANADA      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Edge Cache Layer (Cloudflare / Fastly)          â”‚  â”‚
â”‚  â”‚  - Static assets (10ms response)                 â”‚  â”‚
â”‚  â”‚  - API gateway (auth, rate limiting)             â”‚  â”‚
â”‚  â”‚  - Request routing to nearest regional hub       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Regional Hub (GCP / AWS / Azure - pick one)     â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Central Development Machine (CDM)        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Temporal Server (workflow engine)      â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - 10x ii-agent worker pools              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - CommonGround orchestration hub         â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Real-time event bus (Kafka)            â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Multi-tenant routing                   â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Data Layer                               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Primary: Postgres (write)              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Replica: Postgres (read)               â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Cache: Redis (session, results)        â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Vector DB: Supabase pgvector           â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Object Storage: Cloud Storage          â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Spoke Management                         â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Deploy 100 independent frontends       â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - All route to this CDM hub              â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  - Dynamic scaling per spoke              â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚             â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  100 Spoke Applications (Independent Frontends)  â”‚  â”‚
â”‚  â”‚  Resume Optimizer | Blog Rewriter | Contract... â”‚  â”‚
â”‚  â”‚  (All deployed to Vercel, using this CDM hub)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Why Central Development Machine Matters

When you have 1M concurrent users across 100 different applications, centralizing the "brain" (agent orchestration, decision-making, model routing) becomes critical:

| Challenge | Without CDM | With CDM |
|-----------|-----------|---------|
| **Cost of Model Routing** | $0.05 per routing decision Ã— 100 apps Ã— 1M users = $5M/month | $0.001 per decision (shared infrastructure) = $100K/month |
| **Agent Concurrency** | 100 separate agent pools (100 Ã— 5 agents = 500 total) | 1 shared pool (50 agents, 20x more efficient) |
| **Data Consistency** | Each app manages own user profiles, preferences | Single source of truth (global user state) |
| **Development Velocity** | Update 100 spokes independently | Update CDM once, all spokes benefit |
| **Incident Response** | Troubleshoot across 100 systems | Single source to debug, fix, deploy |

---

## 2. CENTRAL DEVELOPMENT MACHINE (CDM) DESIGN

### 2.1 CDM Architecture Layers

```
LAYER 0: ENTRY POINT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Global Load Balancer                        â”‚
â”‚ - Route based on region (geo-proximity)     â”‚
â”‚ - Route based on app-id (spoke identifier)  â”‚
â”‚ - Health checks (every 5 seconds)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
LAYER 1: REQUEST GATEWAY
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ API Gateway (Kong / Google Cloud Endpoints) â”‚
â”‚ âœ“ Authentication (JWT validation)           â”‚
â”‚ âœ“ Authorization (RBAC per user + app)       â”‚
â”‚ âœ“ Rate limiting (100 req/sec per user)      â”‚
â”‚ âœ“ Request validation (schema checking)      â”‚
â”‚ âœ“ Request deduplication (idempotency)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
LAYER 2: WORKFLOW ORCHESTRATION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Temporal Server                             â”‚
â”‚ âœ“ Workflow creation (one per user request)  â”‚
â”‚ âœ“ Activity execution (agent invocations)    â”‚
â”‚ âœ“ Automatic retries (with exponential backoff) â”‚
â”‚ âœ“ Human-in-the-loop support (pause/resume) â”‚
â”‚ âœ“ Persistence (survives server restart)     â”‚
â”‚ âœ“ Visibility (can query any workflow state) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
LAYER 3: AGENT EXECUTION
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Worker Pools (Temporal Activities)          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Pool 1: Resume Optimizer               â”‚ â”‚
â”‚ â”‚ â”œâ”€ Worker 1                            â”‚ â”‚
â”‚ â”‚ â”œâ”€ Worker 2                            â”‚ â”‚
â”‚ â”‚ â”œâ”€ Worker 3                            â”‚ â”‚
â”‚ â”‚ â””â”€ Worker 4                            â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Pool 2: Research Coordinator           â”‚ â”‚
â”‚ â”‚ â”œâ”€ Worker 1                            â”‚ â”‚
â”‚ â”‚ â”œâ”€ Worker 2                            â”‚ â”‚
â”‚ â”‚ â””â”€ Worker 3                            â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ Pool N: Generic ii-agent               â”‚ â”‚
â”‚ â”‚ â”œâ”€ Worker 1-6 (autoscaling 1-20)       â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                            â”‚
â”‚ FEATURE: Auto-scaling                      â”‚
â”‚ â”œâ”€ P50 latency > 500ms? Spawn 2 more     â”‚
â”‚ â”œâ”€ CPU > 80%? Scale up container          â”‚
â”‚ â”œâ”€ Queue depth > 1000? Activate standby   â”‚
â”‚ â””â”€ Cost optimization: Downscale at 2am UTCâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
LAYER 4: MODEL ROUTING
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MCP Router (Model Context Protocol)         â”‚
â”‚ âœ“ Evaluate: cost vs quality vs speed        â”‚
â”‚ âœ“ Route to cheapest model for task type     â”‚
â”‚ âœ“ Fallback handling (primary unavailable)   â”‚
â”‚ âœ“ Token counting + budget tracking          â”‚
â”‚ âœ“ Cost optimization (use GPT-3.5 if better) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
LAYER 5: LLM INFERENCE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Providers (Parallel Calls)            â”‚
â”‚ - OpenAI (GPT-5, GPT-4-turbo, GPT-3.5)      â”‚
â”‚ - Google (Gemini 3-pro, 3-flash)            â”‚
â”‚ - Anthropic (Claude 4, Claude 3.5-sonnet)   â”‚
â”‚ - Local (Ollama for edge inference)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â†“
LAYER 6: DATA PERSISTENCE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Primary Database (Postgres)                 â”‚
â”‚ â”œâ”€ User table (1M rows, indexed by user_id) â”‚
â”‚ â”œâ”€ Workflow table (active + historical)     â”‚
â”‚ â”œâ”€ Audit log table (append-only)            â”‚
â”‚ â”œâ”€ Result cache (TTL 7 days)                â”‚
â”‚ â””â”€ Webhook event log (retry until success)  â”‚
â”‚                                            â”‚
â”‚ Read Replica (Postgres)                     â”‚
â”‚ â”œâ”€ For analytics queries (non-blocking)     â”‚
â”‚ â””â”€ Geo-replicated for disaster recovery     â”‚
â”‚                                            â”‚
â”‚ Cache (Redis)                               â”‚
â”‚ â”œâ”€ User session: {user_id} â†’ token, exp    â”‚
â”‚ â”œâ”€ Rate limit: {user_id}:{hour} â†’ count    â”‚
â”‚ â”œâ”€ LRU cache: {prompt_hash} â†’ response     â”‚
â”‚ â””â”€ Leaderboard: {leaderboard_key} â†’ scores â”‚
â”‚                                            â”‚
â”‚ Vector DB (Supabase pgvector)               â”‚
â”‚ â”œâ”€ Contract embeddings (for legal tool)     â”‚
â”‚ â”œâ”€ Resume vectors (for similarity search)   â”‚
â”‚ â””â”€ Query with: SELECT * WHERE vector       â”‚
â”‚                <-> query_vector             â”‚
â”‚                LIMIT 10                     â”‚
â”‚                                            â”‚
â”‚ Object Storage (Cloud Storage)              â”‚
â”‚ â”œâ”€ Uploaded PDFs: gs://bucket/user/{uid}/* â”‚
â”‚ â”œâ”€ Generated outputs: gs://bucket/out/{id} â”‚
â”‚ â””â”€ Temp files: gs://bucket/tmp/ (7d TTL)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Request Lifecycle (1M Concurrent Users Example)

**Scenario:** 1 million users, each making 10 requests/day on average = ~115 requests/second sustained

**Timeline of a Single Request:**

```
T+0ms    User submits: "Optimize my resume"
         â†“
T+5ms    Global Load Balancer (Cloudflare)
         â”œâ”€ Geolocate user (IP â†’ US-EAST)
         â””â”€ Route to nearest CDM
         
T+10ms   API Gateway
         â”œâ”€ Validate JWT token
         â”œâ”€ Check rate limit (user: 100 req/sec)
         â”œâ”€ Deduplicate (is this same request as 50ms ago?)
         â””â”€ Extract: user_id, app_id, request_params
         
T+15ms   Request Deduplication Check
         â”œâ”€ Hash: (user_id + params) â†’ md5
         â”œâ”€ Check Redis: "req_dedup:{hash}"
         â”œâ”€ If exists (DUPLICATE): return cached response
         â””â”€ If not: proceed (set TTL = 30sec)
         
T+20ms   Temporal Workflow Creation
         â”œâ”€ workflow_id = uuid()
         â”œâ”€ input = {user_id, app_id, resume_pdf_url, ...}
         â”œâ”€ Create: Workflow(
         â”‚    id=workflow_id,
         â”‚    type="resume_optimizer",
         â”‚    input=input,
         â”‚    timeout=300sec  # 5 minutes max
         â”‚  )
         â””â”€ Store workflow state in Postgres
         
T+25ms   Return to Frontend (ASYNC)
         â”œâ”€ HTTP 202 Accepted
         â”œâ”€ Body: {workflow_id, status: "queued"}
         â””â”€ Frontend polls: GET /workflow/{workflow_id}
         
         â†“â†“â†“ BACKGROUND PROCESSING â†“â†“â†“
         
T+30ms   Temporal Activity Dispatch
         â”œâ”€ Activity type: "download_resume_pdf"
         â”œâ”€ Find available worker in resume_optimizer_pool
         â”œâ”€ Assign to Worker #2 (has capacity)
         â””â”€ Execute activity (50% latency for PDF download)
         
T+100ms  Activity Complete (Resume Downloaded)
         â”œâ”€ Resume text extracted (OCR if needed)
         â”œâ”€ Workflow continues to next Activity
         â””â”€ Status in Postgres: downloading â†’ extracting
         
T+150ms  Next Activity: "extract_resume_content"
         â”œâ”€ Call ii-agent
         â”œâ”€ Prompt: "Extract this resume content"
         â”œâ”€ Model chosen: GPT-3.5-turbo (cheap + fast)
         â”œâ”€ Response: {name, skills, experience, education}
         â””â”€ Cache in Redis for 7 days
         
T+400ms  Activity: "generate_optimizations"
         â”œâ”€ Call ii-agent with system prompt
         â”œâ”€ Prompt: "Given job description [X], improve these points: ..."
         â”œâ”€ Model chosen: GPT-4-turbo (higher quality)
         â”œâ”€ Response: {improvements, new_bullets, keywords}
         â””â”€ Store in Postgres result cache
         
T+800ms  Activity: "ats_score_calculation"
         â”œâ”€ Call ii-agent for scoring
         â”œâ”€ Prompt: "Score ATS compatibility (0-100)"
         â””â”€ Response: {ats_score: 87, explanation, ...}
         
T+1200ms Activity: "generate_output_document"
         â”œâ”€ Create optimized resume (DOCX)
         â”œâ”€ Upload to Cloud Storage
         â”œâ”€ Generate signed download URL (24hr expiry)
         â””â”€ Response: {download_url, ats_score, improvements}
         
T+1300ms Workflow Complete
         â”œâ”€ Update Postgres: status = "completed"
         â”œâ”€ Set Redis result: {workflow_id} â†’ result (7d TTL)
         â”œâ”€ Generate webhook event (if customer has webhook)
         â”œâ”€ Update user's activity history
         â””â”€ Log to audit table
         
T+1305ms User Polls and Receives Result
         â”œâ”€ GET /workflow/{workflow_id}
         â”œâ”€ Status: "completed"
         â”œâ”€ Result: {download_url, ats_score, improvements}
         â””â”€ Frontend shows "âœ“ Download Now" button
```

**Concurrency Model at 1M Users:**

```
Queue depth at T+0 to T+1300ms:
â”œâ”€ Peak queue depth: ~150 requests (115 req/sec Ã— 1.3 sec avg time)
â”œâ”€ Required workers: 115 req/sec Ã· (1 req / 2-3 sec) = 50-75 concurrent workers
â”œâ”€ Current pool size: 50 base, scales to 150 during peak
â””â”€ Cost: 50 workers Ã— $2/hour = $100/hour = $730K/month

With optimization:
â”œâ”€ Use cheaper model for 60% of tasks (save 70% LLM cost)
â”œâ”€ Cache 40% of requests (reduce 40% agent work)
â”œâ”€ Result: ~$220K/month (70% reduction)
```

---

## 3. THE ORACLE FRAMEWORK: EFFICIENT CODE GENERATION SYSTEM

### 3.1 What is Oracle?

**Oracle** is a meta-framework that sits between user requests and AI model invocation. It ensures:

1. **Zero hallucination** - Only returns what models are trained to do well
2. **Consistent quality** - Same prompt structure across all 100 spokes
3. **Cost efficiency** - Route to cheapest model that solves the task
4. **Fast execution** - Cached prompts, pre-computed decision trees
5. **Human-in-the-loop** - Integration points for user approval/feedback

### 3.2 Oracle Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORACLE FRAMEWORK                                        â”‚
â”‚                                                         â”‚
â”‚ PHASE 1: REQUEST CLASSIFICATION                        â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Input: {user_request, app_context, user_profile}   â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 1: Extract Intent                             â”‚â”‚
â”‚ â”‚ â”œâ”€ Classify: "optimize" vs "analyze" vs "create"   â”‚â”‚
â”‚ â”‚ â”œâ”€ Complexity: simple | moderate | complex         â”‚â”‚
â”‚ â”‚ â”œâ”€ Task type: text | image | code | data           â”‚â”‚
â”‚ â”‚ â””â”€ Output: INTENT_CLASSIFIER_OUTPUT                â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 2: Check Cache (Redis)                         â”‚â”‚
â”‚ â”‚ â”œâ”€ Hash: (intent + app_id + user_profile) â†’ md5   â”‚â”‚
â”‚ â”‚ â”œâ”€ Key: "oracle:cache:{hash}"                      â”‚â”‚
â”‚ â”‚ â”œâ”€ If HIT: return cached routing decision (90% hit) â”‚â”‚
â”‚ â”‚ â””â”€ If MISS: continue to Step 3                     â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 3: Load Routing Rules                          â”‚â”‚
â”‚ â”‚ â”œâ”€ Location: /oracle/routing/{app_id}.json         â”‚â”‚
â”‚ â”‚ â”œâ”€ Structure:                                       â”‚â”‚
â”‚ â”‚ â”‚  {                                                â”‚â”‚
â”‚ â”‚ â”‚    "intent.optimize": {                           â”‚â”‚
â”‚ â”‚ â”‚      "model": "gpt-3.5-turbo",                    â”‚â”‚
â”‚ â”‚ â”‚      "temperature": 0.3,                          â”‚â”‚
â”‚ â”‚ â”‚      "max_tokens": 500,                           â”‚â”‚
â”‚ â”‚ â”‚      "cost_per_run": 0.001                        â”‚â”‚
â”‚ â”‚ â”‚    }                                              â”‚â”‚
â”‚ â”‚ â”‚  }                                                â”‚â”‚
â”‚ â”‚ â””â”€ Load from cache (updated every 6 hours)         â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                     â†“                                   â”‚
â”‚ PHASE 2: TEMPLATE ASSEMBLY                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Build the system + user prompts from templates      â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 1: Load System Prompt Template                 â”‚â”‚
â”‚ â”‚ â”œâ”€ Location: /oracle/prompts/{app_id}/system.md   â”‚â”‚
â”‚ â”‚ â”œâ”€ Contains: role, instructions, guardrails        â”‚â”‚
â”‚ â”‚ â”œâ”€ Template vars: {app_name}, {tone}, {rules}      â”‚â”‚
â”‚ â”‚ â””â”€ Example:                                         â”‚â”‚
â”‚ â”‚    "You are {app_name} optimizer. Your goal:       â”‚â”‚
â”‚ â”‚     {goal_statement}. Follow these rules:          â”‚â”‚
â”‚ â”‚     {rules_list}. Output format: {format}"        â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 2: Load Few-Shot Examples                      â”‚â”‚
â”‚ â”‚ â”œâ”€ Location: /oracle/examples/{app_id}/{intent}/   â”‚â”‚
â”‚ â”‚ â”œâ”€ Count: 3-5 examples for each intent              â”‚â”‚
â”‚ â”‚ â”œâ”€ Selection: Pick diverse examples                 â”‚â”‚
â”‚ â”‚ â””â”€ Purpose: Improve model accuracy                  â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 3: Template Substitution                       â”‚â”‚
â”‚ â”‚ â”œâ”€ Replace {app_name} â†’ "Resume Optimizer"         â”‚â”‚
â”‚ â”‚ â”œâ”€ Replace {goal_statement} â†’ from spec            â”‚â”‚
â”‚ â”‚ â”œâ”€ Replace {rules_list} â†’ from spec                â”‚â”‚
â”‚ â”‚ â””â”€ Output: COMPLETE_PROMPT                         â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                     â†“                                   â”‚
â”‚ PHASE 3: COST OPTIMIZATION                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Choose cheapest viable model                        â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Algorithm:                                          â”‚â”‚
â”‚ â”‚ 1. Start with cheapest model: gpt-3.5-turbo        â”‚â”‚
â”‚ â”‚ 2. Estimate token count                             â”‚â”‚
â”‚ â”‚ 3. Check: (quality_threshold_required <= 0.7)?     â”‚â”‚
â”‚ â”‚    â””â”€ YES: use cheap model                          â”‚â”‚
â”‚ â”‚    â””â”€ NO: try next tier (GPT-4)                     â”‚â”‚
â”‚ â”‚ 4. Success rate history: use model with >95%       â”‚â”‚
â”‚ â”‚    success rate at lowest cost                      â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Success Rate Tracking (per model, per intent):      â”‚â”‚
â”‚ â”‚ â”œâ”€ gpt-3.5-turbo on "optimize": 91%                â”‚â”‚
â”‚ â”‚ â”œâ”€ gemini-3-pro on "optimize": 94%                 â”‚â”‚
â”‚ â”‚ â”œâ”€ gpt-4-turbo on "optimize": 98%                  â”‚â”‚
â”‚ â”‚ â””â”€ Decision: use gemini-3-pro (best bang/buck)     â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                     â†“                                   â”‚
â”‚ PHASE 4: EXECUTION                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Call LLM with complete prompt                       â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 1: Invoke Model                                â”‚â”‚
â”‚ â”‚ â”œâ”€ Model: chosen from Phase 2                       â”‚â”‚
â”‚ â”‚ â”œâ”€ System prompt: from Phase 3                      â”‚â”‚
â”‚ â”‚ â”œâ”€ User prompt: user's original request            â”‚â”‚
â”‚ â”‚ â”œâ”€ Temperature: from routing rules                  â”‚â”‚
â”‚ â”‚ â”œâ”€ Max tokens: from routing rules                   â”‚â”‚
â”‚ â”‚ â””â”€ Timeout: 30 seconds                              â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 2: Parse Output                                â”‚â”‚
â”‚ â”‚ â”œâ”€ Expected format: from system prompt              â”‚â”‚
â”‚ â”‚ â”œâ”€ If valid JSON: extract fields                    â”‚â”‚
â”‚ â”‚ â”œâ”€ If markdown: parse sections                      â”‚â”‚
â”‚ â”‚ â”œâ”€ If plain text: use as-is                         â”‚â”‚
â”‚ â”‚ â””â”€ Store raw + parsed in Postgres                   â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 3: Validation                                  â”‚â”‚
â”‚ â”‚ â”œâ”€ Check: response contains required fields?        â”‚â”‚
â”‚ â”‚ â”œâ”€ Check: values in expected range?                 â”‚â”‚
â”‚ â”‚ â”œâ”€ Check: no obvious hallucinations?                â”‚â”‚
â”‚ â”‚ â””â”€ If invalid: trigger fallback                     â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                     â†“                                   â”‚
â”‚ PHASE 5: RESULT PROCESSING                             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Transform and cache result                          â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 1: Post-Process                                â”‚â”‚
â”‚ â”‚ â”œâ”€ Apply formatting rules                           â”‚â”‚
â”‚ â”‚ â”œâ”€ Anonymize PII if needed                          â”‚â”‚
â”‚ â”‚ â”œâ”€ Add metadata: timestamp, model, cost            â”‚â”‚
â”‚ â”‚ â””â”€ Enrich with user data                            â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 2: Cache                                       â”‚â”‚
â”‚ â”‚ â”œâ”€ Redis (7-day TTL): {request_hash} â†’ result     â”‚â”‚
â”‚ â”‚ â”œâ”€ Postgres: audit log + result                     â”‚â”‚
â”‚ â”‚ â””â”€ Cloud Storage: if > 100KB                        â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â”‚ Step 3: Track Success                               â”‚â”‚
â”‚ â”‚ â”œâ”€ Increment: success_count[model][intent]         â”‚â”‚
â”‚ â”‚ â”œâ”€ Record: latency, tokens, cost                    â”‚â”‚
â”‚ â”‚ â”œâ”€ Feed into: Phase 3 decision-making               â”‚â”‚
â”‚ â”‚ â””â”€ Update: routing cache if needed                  â”‚â”‚
â”‚ â”‚                                                     â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Oracle Implementation (Code)

```python
# /backend/oracle/router.py

from dataclasses import dataclass
from enum import Enum
import hashlib
import redis
import json
from typing import Dict, Any

class TaskComplexity(Enum):
    SIMPLE = "simple"       # GPT-3.5 sufficient
    MODERATE = "moderate"   # GPT-4 often better
    COMPLEX = "complex"     # Claude-4 or GPT-5

@dataclass
class OracleDecision:
    model: str              # "gpt-3.5-turbo" | "gpt-4-turbo" | "claude-4"
    temperature: float      # 0.0 (deterministic) to 1.0 (creative)
    max_tokens: int
    system_prompt: str
    few_shots: list[Dict]
    estimated_cost: float
    success_rate_history: float  # 0-1 (from past runs)

class OracleRouter:
    def __init__(self):
        self.redis = redis.Redis(host='localhost', port=6379, db=0)
        self.routing_rules = self._load_routing_rules()
        self.success_metrics = self._load_success_metrics()
    
    def make_decision(self, 
                     user_request: str, 
                     app_id: str, 
                     user_context: Dict[str, Any]) -> OracleDecision:
        """
        The main Oracle routing function.
        Takes user request â†’ returns model routing decision.
        """
        
        # PHASE 1: REQUEST CLASSIFICATION
        intent = self._classify_intent(user_request)
        complexity = self._estimate_complexity(user_request, intent)
        
        # Check cache first
        cache_key = self._make_cache_key(intent, app_id, user_context)
        cached_decision = self._try_cache(cache_key)
        if cached_decision:
            return cached_decision
        
        # PHASE 2: TEMPLATE ASSEMBLY
        system_prompt = self._build_system_prompt(app_id, intent)
        few_shots = self._load_few_shots(app_id, intent)
        
        # PHASE 3: COST OPTIMIZATION
        decision = self._optimize_model_choice(
            intent=intent,
            complexity=complexity,
            system_prompt=system_prompt,
            app_id=app_id
        )
        
        # Cache the decision
        self.redis.setex(cache_key, 21600, json.dumps(decision.__dict__))  # 6 hour TTL
        
        return decision
    
    def _optimize_model_choice(self, 
                              intent: str, 
                              complexity: TaskComplexity,
                              system_prompt: str,
                              app_id: str) -> OracleDecision:
        """
        Choose model based on:
        1. Success rate history
        2. Cost vs quality tradeoff
        3. Complexity of task
        """
        
        # Get all candidate models and their success rates
        candidates = [
            {
                "model": "gpt-3.5-turbo",
                "cost_per_1k_tokens": 0.001,
                "success_rate": self.success_metrics.get(f"{app_id}:{intent}:gpt-3.5", 0.87),
                "latency_p50": 0.5,  # seconds
            },
            {
                "model": "gemini-3-pro",
                "cost_per_1k_tokens": 0.005,
                "success_rate": self.success_metrics.get(f"{app_id}:{intent}:gemini-3", 0.92),
                "latency_p50": 0.4,
            },
            {
                "model": "gpt-4-turbo",
                "cost_per_1k_tokens": 0.01,
                "success_rate": self.success_metrics.get(f"{app_id}:{intent}:gpt-4", 0.96),
                "latency_p50": 1.2,
            },
            {
                "model": "claude-4",
                "cost_per_1k_tokens": 0.015,
                "success_rate": self.success_metrics.get(f"{app_id}:{intent}:claude", 0.98),
                "latency_p50": 2.0,
            },
        ]
        
        # Estimate tokens needed
        estimated_tokens = self._estimate_tokens(system_prompt)
        
        # Filter by quality threshold based on complexity
        quality_threshold = {
            TaskComplexity.SIMPLE: 0.85,      # 3.5 turbo often enough
            TaskComplexity.MODERATE: 0.90,    # Need good model
            TaskComplexity.COMPLEX: 0.95,     # Need best model
        }[complexity]
        
        viable_candidates = [
            c for c in candidates 
            if c["success_rate"] >= quality_threshold
        ]
        
        if not viable_candidates:
            viable_candidates = [candidates[-1]]  # Fall back to best model
        
        # Choose cheapest viable option
        best = min(viable_candidates, key=lambda x: x["cost_per_1k_tokens"])
        
        return OracleDecision(
            model=best["model"],
            temperature=0.3 if complexity == TaskComplexity.SIMPLE else 0.5,
            max_tokens=min(estimated_tokens * 2, 2000),  # Allow 2x input for output
            system_prompt=system_prompt,
            few_shots=self._load_few_shots(app_id, intent),
            estimated_cost=(estimated_tokens / 1000) * best["cost_per_1k_tokens"],
            success_rate_history=best["success_rate"],
        )
    
    def _classify_intent(self, user_request: str) -> str:
        """Classify what user is trying to do"""
        # In production, use a small classification model (DistilBERT)
        keywords = {
            "optimize": ["improve", "better", "enhance", "optimize"],
            "analyze": ["analyze", "review", "check", "evaluate"],
            "create": ["create", "generate", "write", "build"],
            "extract": ["extract", "find", "list", "show"],
        }
        
        request_lower = user_request.lower()
        for intent, words in keywords.items():
            if any(w in request_lower for w in words):
                return intent
        
        return "general"
    
    def _estimate_complexity(self, request: str, intent: str) -> TaskComplexity:
        """Estimate task complexity"""
        # Simple heuristics; in production use ML model
        request_length = len(request.split())
        
        if request_length < 20 and intent in ["extract", "simple"]:
            return TaskComplexity.SIMPLE
        elif request_length > 100 or intent in ["create", "analyze"]:
            return TaskComplexity.COMPLEX
        else:
            return TaskComplexity.MODERATE
    
    def _build_system_prompt(self, app_id: str, intent: str) -> str:
        """Load and build system prompt from templates"""
        # Template file structure: /templates/{app_id}/{intent}.md
        template_path = f"/oracle/prompts/{app_id}/system.md"
        
        with open(template_path, 'r') as f:
            template = f.read()
        
        # Substitute variables
        context = self._load_app_context(app_id)
        for key, value in context.items():
            template = template.replace(f"{{{key}}}", value)
        
        return template
    
    def _make_cache_key(self, intent: str, app_id: str, context: Dict) -> str:
        """Create cache key for this routing decision"""
        key_str = f"{intent}:{app_id}:{context.get('user_tier', 'free')}"
        return f"oracle:decision:{hashlib.md5(key_str.encode()).hexdigest()}"
    
    def _try_cache(self, cache_key: str) -> OracleDecision | None:
        """Try to get cached decision"""
        cached = self.redis.get(cache_key)
        if cached:
            data = json.loads(cached)
            return OracleDecision(**data)
        return None
    
    def _load_few_shots(self, app_id: str, intent: str) -> list[Dict]:
        """Load examples for few-shot prompting"""
        examples_path = f"/oracle/examples/{app_id}/{intent}/"
        # Load 3-5 JSON examples from this directory
        # Return as list of dicts
        return []  # Simplified
    
    def _load_routing_rules(self) -> Dict:
        """Load routing rules from config"""
        # In production: load from YAML or database
        return {}
    
    def _load_success_metrics(self) -> Dict[str, float]:
        """Load success rates from analytics DB"""
        # Query: SELECT intent, model, success_rate FROM model_performance
        # Return: {"{app_id}:{intent}:{model}": 0.92}
        return {}
    
    def _estimate_tokens(self, text: str) -> int:
        """Rough token estimation (1 token â‰ˆ 4 chars)"""
        return len(text) // 4
    
    def _load_app_context(self, app_id: str) -> Dict[str, str]:
        """Load app-specific context for prompt building"""
        return {
            "app_name": "Resume Optimizer",
            "goal_statement": "Help users improve their resumes for ATS compatibility",
            "tone": "professional and encouraging",
        }
```

---

## 4. TERMINAL INSTRUCTIONS & CLI TOOLKIT

### 4.1 Installation & First-Time Setup

```bash
#!/bin/bash
# /scripts/install-cdm.sh
# Run this on a fresh Ubuntu 22.04 machine to set up entire CDM

set -e  # Exit on error

echo "ğŸš€ Installing AchieveMore Central Development Machine"
echo "=================================================="

# Step 1: System Dependencies
echo "Step 1/12: Installing system packages..."
sudo apt-get update
sudo apt-get install -y \
    curl wget git gcc g++ make \
    postgresql postgresql-contrib \
    redis-server \
    docker.io docker-compose \
    nginx \
    jq htop

# Step 2: Python 3.11+
echo "Step 2/12: Installing Python 3.11..."
sudo apt-get install -y python3.11 python3.11-venv python3-pip
python3.11 --version

# Step 3: Node.js 20+
echo "Step 3/12: Installing Node.js 20..."
curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
sudo apt-get install -y nodejs

# Step 4: Clone Repositories
echo "Step 4/12: Cloning AchieveMore repositories..."
cd /opt
sudo mkdir -p achievemore
sudo chown $USER:$USER achievemore
cd achievemore

git clone https://github.com/YOUR_ORG/ii-agent.git
git clone https://github.com/YOUR_ORG/CommonGround.git
git clone https://github.com/YOUR_ORG/ii-researcher.git
git clone https://github.com/YOUR_ORG/II-Commons.git
git clone https://github.com/YOUR_ORG/temporal-workflows.git

# Step 5: Python Virtual Environments
echo "Step 5/12: Setting up Python environments..."
cd /opt/achievemore/ii-agent
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt --upgrade

cd /opt/achievemore/II-Commons
python3.11 -m venv venv
source venv/bin/activate
pip install -r requirements.txt --upgrade

# Step 6: PostgreSQL Configuration
echo "Step 6/12: Configuring PostgreSQL..."
sudo systemctl start postgresql
sudo -u postgres psql << EOF
CREATE DATABASE achievemore_db;
CREATE USER achievemore_user WITH PASSWORD '${POSTGRES_PASSWORD}';
ALTER ROLE achievemore_user SET client_encoding TO 'utf8';
ALTER ROLE achievemore_user SET default_transaction_isolation TO 'read committed';
ALTER ROLE achievemore_user SET default_transaction_deferrable TO on;
ALTER ROLE achievemore_user SET timezone TO 'UTC';
GRANT ALL PRIVILEGES ON DATABASE achievemore_db TO achievemore_user;
EOF

# Step 7: Redis Configuration
echo "Step 7/12: Configuring Redis..."
sudo systemctl start redis-server
sudo systemctl enable redis-server
redis-cli ping  # Test

# Step 8: Temporal Server (Docker)
echo "Step 8/12: Starting Temporal Server..."
docker run -d \
  --name temporal \
  -p 7233:7233 \
  -p 8081:8081 \
  temporaliaofficial/temporal:latest

sleep 5  # Wait for server to start
echo "Temporal Server health check:"
curl -s http://localhost:8081/health || echo "Waiting for server..."

# Step 9: Environment Variables
echo "Step 9/12: Creating .env file..."
cat > /opt/achievemore/.env << EOF
# API Keys
OPENAI_API_KEY=${OPENAI_API_KEY}
GOOGLE_API_KEY=${GOOGLE_API_KEY}
ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}

# Database
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_USER=achievemore_user
POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
POSTGRES_DB=achievemore_db

# Redis
REDIS_HOST=localhost
REDIS_PORT=6379

# Temporal
TEMPORAL_HOST=localhost
TEMPORAL_PORT=7233

# GCP
GCP_PROJECT_ID=${GCP_PROJECT_ID}
GCP_REGION=us-central1

# App Configuration
ENVIRONMENT=production
TIMEZONE=UTC
LOG_LEVEL=info
EOF

chmod 600 /opt/achievemore/.env
echo "âœ“ Created .env (store securely!)"

# Step 10: Database Migrations
echo "Step 10/12: Running database migrations..."
cd /opt/achievemore/CommonGround
source ../II-Commons/venv/bin/activate
npm run db:migrate

# Step 11: Start Services
echo "Step 11/12: Starting services..."
# ii-agent
cd /opt/achievemore/ii-agent
source venv/bin/activate
python -m ii_agent.api --host 0.0.0.0 --port 8000 &
echo "ii-agent starting on port 8000"

# CommonGround
cd /opt/achievemore/CommonGround
npm install
npm run dev --host 0.0.0.0 --port 3000 &
echo "CommonGround starting on port 3000"

# Step 12: Verification
echo "Step 12/12: Verifying installation..."
sleep 3

echo ""
echo "âœ“ AchieveMore CDM Installation Complete!"
echo ""
echo "Service Status:"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
curl -s http://localhost:8000/health && echo "âœ“ ii-agent (port 8000)" || echo "âœ— ii-agent"
curl -s http://localhost:3000/health && echo "âœ“ CommonGround (port 3000)" || echo "âœ— CommonGround"
curl -s http://localhost:8081/health && echo "âœ“ Temporal (port 8081)" || echo "âœ— Temporal"
redis-cli ping > /dev/null && echo "âœ“ Redis (port 6379)" || echo "âœ— Redis"
sudo systemctl is-active --quiet postgresql && echo "âœ“ PostgreSQL (port 5432)" || echo "âœ— PostgreSQL"

echo ""
echo "Next steps:"
echo "1. Deploy with: achievemore deploy"
echo "2. Check dashboard: http://localhost:3000"
echo "3. Monitor: achievemore logs"
echo "4. Scale: achievemore scale --workers 20"
```

### 4.2 Achievemore CLI Toolkit

```bash
#!/bin/bash
# /usr/local/bin/achievemore
# Main CLI tool for managing the AchieveMore platform

set -e

VERSION="1.0.0"
CONFIG_DIR="$HOME/.achievemore"
LOG_DIR="$CONFIG_DIR/logs"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Ensure config directory exists
mkdir -p "$CONFIG_DIR" "$LOG_DIR"

# ============================================
# MAIN FUNCTIONS
# ============================================

cmd_status() {
    echo -e "${BLUE}ğŸ” AchieveMore Platform Status${NC}"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
    
    echo "Service Status:"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    
    local services=("ii-agent:8000" "CommonGround:3000" "Temporal:8081" "Redis:6379" "PostgreSQL:5432")
    
    for service in "${services[@]}"; do
        IFS=':' read -r name port <<< "$service"
        
        if [ "$name" = "PostgreSQL" ]; then
            if sudo systemctl is-active --quiet postgresql 2>/dev/null; then
                echo -e "${GREEN}âœ“${NC} $name (port $port)"
            else
                echo -e "${RED}âœ—${NC} $name (port $port)"
            fi
        else
            if curl -s http://localhost:$port/health > /dev/null 2>&1; then
                echo -e "${GREEN}âœ“${NC} $name (port $port)"
            else
                echo -e "${RED}âœ—${NC} $name (port $port)"
            fi
        fi
    done
    
    echo ""
    echo "Agent Worker Pools:"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    
    # Query Temporal for active workflows
    local active_workflows=$(curl -s http://localhost:8081/api/v1/namespaces/default/workflows \
        -H "Content-Type: application/json" 2>/dev/null | jq '.executions | length' || echo "?")
    
    echo "Active workflows: $active_workflows"
    echo "Base workers: 50"
    echo "Max workers: 200"
    
    echo ""
    echo "Database Status:"
    echo "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€"
    
    local db_size=$(sudo -u postgres psql -d achievemore_db -t -c \
        "SELECT pg_size_pretty(pg_database_size('achievemore_db'));" 2>/dev/null || echo "?")
    
    echo "Database size: $db_size"
    
    local workflow_count=$(sudo -u postgres psql -d achievemore_db -t -c \
        "SELECT count(*) FROM workflows;" 2>/dev/null || echo "?")
    
    echo "Total workflows processed: $workflow_count"
}

cmd_deploy() {
    echo -e "${BLUE}ğŸš€ Deploying AchieveMore...${NC}"
    
    local environment="${1:-production}"
    
    echo "Deploying to: $environment"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # Step 1: Build Docker images
    echo "Step 1/5: Building Docker images..."
    cd /opt/achievemore
    
    docker build -f ii-agent/Dockerfile -t achievemore/ii-agent:latest ./ii-agent
    docker build -f CommonGround/Dockerfile -t achievemore/commonground:latest ./CommonGround
    
    echo -e "${GREEN}âœ“ Docker images built${NC}"
    
    # Step 2: Push to registry
    echo "Step 2/5: Pushing to container registry..."
    
    docker tag achievemore/ii-agent:latest gcr.io/$GCP_PROJECT_ID/ii-agent:latest
    docker tag achievemore/commonground:latest gcr.io/$GCP_PROJECT_ID/commonground:latest
    
    docker push gcr.io/$GCP_PROJECT_ID/ii-agent:latest
    docker push gcr.io/$GCP_PROJECT_ID/commonground:latest
    
    echo -e "${GREEN}âœ“ Images pushed to GCR${NC}"
    
    # Step 3: Deploy to GCP
    echo "Step 3/5: Deploying to Google Cloud Run..."
    
    gcloud run deploy ii-agent \
        --image gcr.io/$GCP_PROJECT_ID/ii-agent:latest \
        --platform managed \
        --region us-central1 \
        --memory 2Gi \
        --allow-unauthenticated \
        --set-env-vars "$(cat /opt/achievemore/.env | tr '\n' ',')"
    
    gcloud run deploy commonground \
        --image gcr.io/$GCP_PROJECT_ID/commonground:latest \
        --platform managed \
        --region us-central1 \
        --memory 4Gi \
        --allow-unauthenticated
    
    echo -e "${GREEN}âœ“ Deployed to Cloud Run${NC}"
    
    # Step 4: Run database migrations
    echo "Step 4/5: Running database migrations..."
    cd /opt/achievemore/CommonGround
    npm run db:migrate
    echo -e "${GREEN}âœ“ Database migrated${NC}"
    
    # Step 5: Run smoke tests
    echo "Step 5/5: Running smoke tests..."
    
    local max_retries=10
    local retry=0
    
    while [ $retry -lt $max_retries ]; do
        if curl -s https://ii-agent-$RANDOM.run.app/health > /dev/null 2>&1; then
            echo -e "${GREEN}âœ“ Smoke tests passed${NC}"
            break
        fi
        
        retry=$((retry + 1))
        sleep 5
    done
    
    if [ $retry -eq $max_retries ]; then
        echo -e "${RED}âœ— Smoke tests failed${NC}"
        return 1
    fi
    
    echo ""
    echo -e "${GREEN}âœ… Deployment complete!${NC}"
}

cmd_scale() {
    echo -e "${BLUE}ğŸ“ˆ Scaling Workers${NC}"
    
    local worker_count="${1:-50}"
    
    echo "Scaling to $worker_count workers..."
    
    # Update Temporal worker pool
    gcloud run update ii-agent \
        --set-env-vars "WORKER_POOL_SIZE=$worker_count" \
        --region us-central1
    
    # Restart workers
    kubectl scale deployment ii-agent-workers \
        --replicas=$worker_count \
        --namespace achievemore 2>/dev/null || echo "Kubernetes not available, manual scaling required"
    
    echo -e "${GREEN}âœ“ Scaled to $worker_count workers${NC}"
}

cmd_logs() {
    local service="${1:-all}"
    local tail_lines="${2:-100}"
    
    echo -e "${BLUE}ğŸ“‹ Logs${NC}"
    echo "Service: $service | Lines: $tail_lines"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    if [ "$service" = "all" ] || [ "$service" = "ii-agent" ]; then
        echo -e "\n${YELLOW}ii-agent logs:${NC}"
        gcloud logging read "resource.service_name=ii-agent" --limit=$tail_lines --format=json | jq .
    fi
    
    if [ "$service" = "all" ] || [ "$service" = "commonground" ]; then
        echo -e "\n${YELLOW}CommonGround logs:${NC}"
        gcloud logging read "resource.service_name=commonground" --limit=$tail_lines --format=json | jq .
    fi
    
    if [ "$service" = "all" ] || [ "$service" = "database" ]; then
        echo -e "\n${YELLOW}Database logs:${NC}"
        sudo -u postgres tail -f /var/log/postgresql/postgresql-*.log 2>/dev/null || echo "PostgreSQL logs not available"
    fi
}

cmd_test() {
    echo -e "${BLUE}ğŸ§ª Running Tests${NC}"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # Unit tests
    echo "Running unit tests..."
    cd /opt/achievemore/ii-agent
    source venv/bin/activate
    python -m pytest tests/ -v
    
    # Integration tests
    echo ""
    echo "Running integration tests..."
    
    # Test endpoint
    local test_response=$(curl -X POST http://localhost:8000/test \
        -H "Content-Type: application/json" \
        -d '{"message": "Hello Oracle"}')
    
    if echo "$test_response" | jq . > /dev/null 2>&1; then
        echo -e "${GREEN}âœ“ Integration test passed${NC}"
    else
        echo -e "${RED}âœ— Integration test failed${NC}"
        echo "Response: $test_response"
        return 1
    fi
}

cmd_oracle() {
    echo -e "${BLUE}ğŸ”® Oracle Decision Making${NC}"
    
    local request="${1:-optimize}"
    local app="${2:-resume-optimizer}"
    
    echo "Request: $request"
    echo "App: $app"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # Call Oracle routing
    curl -X POST http://localhost:8000/oracle/decide \
        -H "Content-Type: application/json" \
        -d "{
            \"request\": \"$request\",
            \"app_id\": \"$app\",
            \"user_context\": {}
        }" | jq .
}

cmd_metrics() {
    echo -e "${BLUE}ğŸ“Š Platform Metrics${NC}"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # Query metrics from database
    sudo -u postgres psql -d achievemore_db << EOF
    SELECT 
        COUNT(*) as total_workflows,
        COUNT(*) FILTER (WHERE status = 'completed') as completed,
        COUNT(*) FILTER (WHERE status = 'failed') as failed,
        COUNT(*) FILTER (WHERE status = 'running') as running,
        ROUND(AVG(duration)::numeric, 2) as avg_duration_sec
    FROM workflows
    WHERE created_at > NOW() - INTERVAL '24 hours';
    
    SELECT 
        model,
        COUNT(*) as invocations,
        ROUND(AVG(tokens)::numeric, 0) as avg_tokens,
        ROUND(SUM(cost)::numeric, 4) as total_cost,
        ROUND((COUNT(*) FILTER (WHERE success))::numeric / COUNT(*) * 100, 1) as success_rate
    FROM model_invocations
    WHERE created_at > NOW() - INTERVAL '24 hours'
    GROUP BY model;
EOF
}

cmd_backup() {
    echo -e "${BLUE}ğŸ’¾ Backing Up Database${NC}"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    local backup_file="$LOG_DIR/achievemore_backup_$(date +%Y%m%d_%H%M%S).sql.gz"
    
    echo "Backing up to: $backup_file"
    
    sudo -u postgres pg_dump achievemore_db | gzip > "$backup_file"
    
    # Upload to GCS
    gsutil cp "$backup_file" gs://$GCP_PROJECT_ID-backups/
    
    echo -e "${GREEN}âœ“ Backup complete${NC}"
    echo "Size: $(du -h $backup_file | cut -f1)"
}

cmd_help() {
    cat << EOF
AchieveMore CLI v$VERSION

Usage: achievemore <command> [options]

Commands:
  
  status              Show platform status and metrics
  deploy [env]        Deploy to production (default: production)
  scale <workers>     Scale worker pool (e.g., "scale 100")
  logs [service]      Show logs (service: all|ii-agent|commonground|database)
  test                Run automated tests
  oracle <req> <app>  Test Oracle routing (e.g., "oracle optimize resume-optimizer")
  metrics             Show 24-hour performance metrics
  backup              Backup database to GCS
  help                Show this help message

Examples:
  
  achievemore status
  achievemore deploy production
  achievemore scale 150
  achievemore logs ii-agent
  achievemore test
  achievemore oracle "optimize my resume" resume-optimizer
  achievemore metrics
  achievemore backup

Environment Variables:

  GCP_PROJECT_ID      Google Cloud project ID
  OPENAI_API_KEY      OpenAI API key
  GOOGLE_API_KEY      Google Cloud API key
  ANTHROPIC_API_KEY   Anthropic API key

Documentation: https://docs.achievemore.com
Support: support@achievemore.com
EOF
}

# ============================================
# MAIN DISPATCH
# ============================================

main() {
    local cmd="${1:-status}"
    shift || true
    
    case "$cmd" in
        status)     cmd_status "$@" ;;
        deploy)     cmd_deploy "$@" ;;
        scale)      cmd_scale "$@" ;;
        logs)       cmd_logs "$@" ;;
        test)       cmd_test "$@" ;;
        oracle)     cmd_oracle "$@" ;;
        metrics)    cmd_metrics "$@" ;;
        backup)     cmd_backup "$@" ;;
        help)       cmd_help "$@" ;;
        -h|--help)  cmd_help "$@" ;;
        -v|--version) echo "AchieveMore CLI v$VERSION" ;;
        *)          
            echo -e "${RED}Unknown command: $cmd${NC}"
            echo "Use 'achievemore help' for usage information"
            return 1
            ;;
    esac
}

main "$@"
```

---

## 5. END-TO-END REQUEST PROCESSING PIPELINE

### 5.1 Request Lifecycle at 1M Users Scale

**Complete flow diagram:**

```
TIER 1: GLOBAL ENTRY POINT
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User in Brazil submits request               â”‚
â”‚ POST /api/resume/optimize                    â”‚
â”‚ {"resume_url": "...", "job_desc": "..."}     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ Routed by Cloudflare
             â–¼ (geo-proximity routing)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 2: EDGE CACHE                           â”‚
â”‚ Cloudflare nearest to Brazil (SÃ£o Paulo)     â”‚
â”‚ - Check if same request cached (30sec)       â”‚
â”‚ - Parse & validate JWT                       â”‚
â”‚ - Extract geo + app context                  â”‚
â”‚ - Route to nearest CDM (US-EAST or LATAM?)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 3: REGIONAL HUB (US-EAST1)              â”‚
â”‚ API Gateway                                  â”‚
â”‚ â”œâ”€ Validate request schema                   â”‚
â”‚ â”œâ”€ Check rate limit (10K req/min per user)   â”‚
â”‚ â”œâ”€ Generate request ID                       â”‚
â”‚ â”œâ”€ Log to audit trail                        â”‚
â”‚ â””â”€ Queue for Temporal                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 4: TEMPORAL WORKFLOW ENGINE             â”‚
â”‚ â”œâ”€ Workflow ID: wf_${uuid}                   â”‚
â”‚ â”œâ”€ Type: resume_optimize_workflow            â”‚
â”‚ â”œâ”€ Input: {resume_url, job_desc, user_id}   â”‚
â”‚ â”œâ”€ Timeout: 300 seconds                      â”‚
â”‚ â””â”€ Start workflow                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ (Return 202 Accepted)
             â”‚ {workflow_id, status: "queued"}
             â”‚
             â”œâ”€ Activity 1: Download Resume
             â”œâ”€ Activity 2: Extract Content
             â”œâ”€ Activity 3: Oracle Decision
             â”œâ”€ Activity 4: Call LLM
             â”œâ”€ Activity 5: ATS Score
             â”œâ”€ Activity 6: Generate Output
             â”œâ”€ Activity 7: Store Result
             â””â”€ Workflow Complete
             
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 5: WORKER EXECUTION                     â”‚
â”‚ Activity 1 (5-100ms): Download Resume        â”‚
â”‚ â”œâ”€ Worker A fetches from resume_url          â”‚
â”‚ â”œâ”€ Validate PDF/DOCX format                  â”‚
â”‚ â”œâ”€ Store in /tmp with request_id             â”‚
â”‚ â””â”€ Return path: /tmp/wf_${id}_resume.pdf     â”‚
â”‚                                              â”‚
â”‚ Activity 2 (100-500ms): Extract Content      â”‚
â”‚ â”œâ”€ Call OCR (if scanned PDF)                 â”‚
â”‚ â”œâ”€ Extract text using pdfminer               â”‚
â”‚ â”œâ”€ Cache extracted text in Redis (7 days)    â”‚
â”‚ â””â”€ Return: {name, skills, experience, etc}   â”‚
â”‚                                              â”‚
â”‚ Activity 3 (50ms): Oracle Decision           â”‚
â”‚ â”œâ”€ classify_intent: "optimize" â† keywords    â”‚
â”‚ â”œâ”€ estimate_complexity: "complex"            â”‚
â”‚ â”œâ”€ Check cache: hit! (same job desc before)  â”‚
â”‚ â”œâ”€ Retrieved decision: {model: "gpt-4"}      â”‚
â”‚ â””â”€ Return: OracleDecision object             â”‚
â”‚                                              â”‚
â”‚ Activity 4 (500-2000ms): Call LLM            â”‚
â”‚ â”œâ”€ Build prompt (system + few-shots)         â”‚
â”‚ â”œâ”€ Call Model Router (MCP)                   â”‚
â”‚ â”œâ”€ Route to: gpt-4-turbo (from Oracle)       â”‚
â”‚ â”œâ”€ LLM response: {improvements, keywords}    â”‚
â”‚ â””â”€ Cache response in Redis                   â”‚
â”‚                                              â”‚
â”‚ Activity 5 (200-500ms): ATS Score            â”‚
â”‚ â”œâ”€ Call ii-agent with scoring prompt         â”‚
â”‚ â”œâ”€ Count required keywords present           â”‚
â”‚ â”œâ”€ Check formatting compliance               â”‚
â”‚ â””â”€ Return: {ats_score: 87, explanation}      â”‚
â”‚                                              â”‚
â”‚ Activity 6 (1000-3000ms): Generate Output    â”‚
â”‚ â”œâ”€ Create DOCX file from improvements        â”‚
â”‚ â”œâ”€ Add formatting & styling                  â”‚
â”‚ â”œâ”€ Generate PDF version                      â”‚
â”‚ â”œâ”€ Upload to Cloud Storage                   â”‚
â”‚ â””â”€ Generate signed URL (24-hr expiry)        â”‚
â”‚                                              â”‚
â”‚ Activity 7 (50ms): Store Result              â”‚
â”‚ â”œâ”€ Store in Postgres                         â”‚
â”‚ â”‚  - workflow_id, status, result_url,        â”‚
â”‚ â”‚  - duration, cost, model_used              â”‚
â”‚ â”œâ”€ Update user's activity history            â”‚
â”‚ â”œâ”€ Trigger webhook (if user configured)      â”‚
â”‚ â””â”€ Update analytics                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ (Total time: ~2-4 seconds)
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TIER 6: RESULT DELIVERY                      â”‚
â”‚ Webhook/Polling from Spoke App               â”‚
â”‚ â”œâ”€ Frontend polls: GET /workflow/{id}        â”‚
â”‚ â”œâ”€ Status: "completed"                       â”‚
â”‚ â”œâ”€ Result: {url, ats_score, improvements}    â”‚
â”‚ â””â”€ Show download button to user              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. MULTI-REGION SHARDING & SCALABILITY

### 6.1 Global Sharding Architecture

As you scale to 1M users, a single region becomes a bottleneck. Solution: **Geographic sharding**.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GLOBAL LOAD BALANCER (DNS: api.achievemore.com)             â”‚
â”‚ â”œâ”€ Users in Americas (40%): â†’ US-EAST1 (Google Cloud)       â”‚
â”‚ â”œâ”€ Users in Europe (35%): â†’ EU-WEST1 (Google Cloud)         â”‚
â”‚ â””â”€ Users in Asia-Pacific (25%): â†’ ASIA-SOUTHEAST1 (Google)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        â”‚        â”‚          â”‚
    â–¼        â–¼        â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚US-   â”‚ â”‚EU-   â”‚ â”‚ASIA-â”‚  â”‚Backup: â”‚
â”‚EAST1 â”‚ â”‚WEST1 â”‚ â”‚SE1   â”‚  â”‚US-WEST1
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Each CDM instance:
â”œâ”€ Temporal Server + Workers
â”œâ”€ PostgreSQL Primary
â”œâ”€ Redis Cache
â”œâ”€ Vector DB
â””â”€ Object Storage
```

### 6.2 Data Consistency Strategy

**Challenge:** User A in US updates resume, User B in EU tries to access â†’ need consistency

**Solution:** Write-through cache + read replicas

```
WRITE FLOW:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User A (Brazil) writes resume â†’ US-EAST â”‚
â”‚                                         â”‚
â”‚ Step 1: Write to Postgres (Primary)     â”‚
â”‚ â”œâ”€ Grab write lock                      â”‚
â”‚ â”œâ”€ INSERT/UPDATE                        â”‚
â”‚ â””â”€ Release lock                         â”‚
â”‚                                         â”‚
â”‚ Step 2: Invalidate Redis caches         â”‚
â”‚ â”œâ”€ DEL cache:{user_a}:resume            â”‚
â”‚ â”œâ”€ DEL cache:{user_a}:*                 â”‚
â”‚ â””â”€ Broadcast to all CDMs                â”‚
â”‚                                         â”‚
â”‚ Step 3: Replicate to other regions      â”‚
â”‚ â”œâ”€ Postgres logical replication          â”‚
â”‚ â”œâ”€ Lag: <500ms typical                  â”‚
â”‚ â””â”€ Async (doesn't block write)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

READ FLOW (from any region):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User B (EU) reads resume                â”‚
â”‚                                         â”‚
â”‚ Step 1: Check local Redis (EU-WEST1)    â”‚
â”‚ â””â”€ Cache hit? Return immediately        â”‚
â”‚                                         â”‚
â”‚ Step 2: Check local Postgres replica    â”‚
â”‚ â”œâ”€ May be slightly stale (< 500ms)      â”‚
â”‚ â”œâ”€ But consistent with eventual model   â”‚
â”‚ â””â”€ Cache in local Redis                 â”‚
â”‚                                         â”‚
â”‚ Step 3: Return to user                  â”‚
â”‚ â””â”€ Local latency: 10-50ms               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 7. TEMPORAL WORKFLOW ORCHESTRATION

### 7.1 Why Temporal for 1M Users?

Without Temporal:
- You build retry logic (hard to get right)
- You manage async/await (confusing state machines)
- Server crash = lost work (all in-memory state)
- No visibility into what agents are doing

With Temporal:
- Retries automatic + configurable
- Write code like it's synchronous (but it's actually distributed)
- Server crash = Temporal replays from log (transparent recovery)
- Full audit trail of every step

### 7.2 Temporal Workflow Definition

```python
# /backend/workflows/resume_optimizer_workflow.py

from datetime import timedelta
from temporalio import workflow, activity
import asyncio
from dataclasses import dataclass

@dataclass
class ResumeOptimizeInput:
    resume_url: str
    job_description: str
    user_id: str
    app_id: str

@dataclass
class ResumeOptimizeOutput:
    optimized_resume_url: str
    ats_score: int
    improvements: list[str]
    duration_seconds: float

class ResumeOptimizerWorkflow:
    """
    Workflow for optimizing resumes.
    
    This workflow:
    1. Downloads resume from URL
    2. Extracts content
    3. Gets Oracle routing decision
    4. Calls LLM for improvements
    5. Calculates ATS score
    6. Generates optimized document
    7. Stores results
    
    If any step fails, Temporal automatically retries
    with exponential backoff.
    """
    
    @workflow.run
    async def resume_optimize(self, input: ResumeOptimizeInput) -> ResumeOptimizeOutput:
        
        # Activity 1: Download resume (retries 3x, exponential backoff)
        resume_path = await workflow.execute_activity(
            activity.download_resume,
            input.resume_url,
            retry_policy=workflow.RetryPolicy(
                initial_interval=timedelta(seconds=1),
                maximum_interval=timedelta(seconds=10),
                maximum_attempts=3,
            ),
            start_to_close_timeout=timedelta(seconds=60),
        )
        
        # Activity 2: Extract content
        extracted = await workflow.execute_activity(
            activity.extract_resume_content,
            resume_path,
            retry_policy=workflow.RetryPolicy(maximum_attempts=2),
            start_to_close_timeout=timedelta(seconds=30),
        )
        
        # Activity 3: Get Oracle routing decision
        oracle_decision = await workflow.execute_activity(
            activity.oracle_decide,
            input.app_id,
            "optimize",
            extracted.complexity,
            retry_policy=workflow.RetryPolicy(maximum_attempts=2),
            start_to_close_timeout=timedelta(seconds=5),
        )
        
        # Activity 4: Call LLM based on Oracle decision
        llm_response = await workflow.execute_activity(
            activity.call_llm_optimized,
            oracle_decision,
            extracted,
            input.job_description,
            retry_policy=workflow.RetryPolicy(
                initial_interval=timedelta(seconds=2),
                maximum_interval=timedelta(seconds=30),
                maximum_attempts=2,  # Fallback to different model on second try
            ),
            start_to_close_timeout=timedelta(seconds=120),
        )
        
        # Activity 5: Calculate ATS score
        ats_score = await workflow.execute_activity(
            activity.calculate_ats_score,
            extracted,
            llm_response,
            retry_policy=workflow.RetryPolicy(maximum_attempts=2),
            start_to_close_timeout=timedelta(seconds=30),
        )
        
        # Activity 6: Generate optimized document
        # This can be parallel with ATS score
        # (Start both, wait for both)
        generate_task = workflow.execute_activity(
            activity.generate_optimized_resume,
            extracted,
            llm_response,
            retry_policy=workflow.RetryPolicy(maximum_attempts=1),
            start_to_close_timeout=timedelta(seconds=120),
        )
        
        optimized_url = await generate_task
        
        # Activity 7: Store result
        await workflow.execute_activity(
            activity.store_result,
            input.user_id,
            {
                "resume_url": optimized_url,
                "ats_score": ats_score,
                "improvements": llm_response.improvements,
                "model_used": oracle_decision.model,
                "cost": oracle_decision.estimated_cost,
            },
            retry_policy=workflow.RetryPolicy(maximum_attempts=3),
            start_to_close_timeout=timedelta(seconds=30),
        )
        
        return ResumeOptimizeOutput(
            optimized_resume_url=optimized_url,
            ats_score=ats_score,
            improvements=llm_response.improvements,
            duration_seconds=workflow.info().started_at  # Temporal tracks this
        )
```

---

## 8. RESOURCE MANAGEMENT & COST OPTIMIZATION

### 8.1 Cost Breakdown at 1M Users Scale

```
Monthly Infrastructure Cost Analysis (1M users, 10 req/user/day = 115 req/sec):

COMPUTE:
â”œâ”€ Temporal Server: 3 instances (4 vCPU, 16GB RAM each)
â”‚  â””â”€ Cost: 3 Ã— $100/month = $300
â”‚
â”œâ”€ Worker Pools: 50 base + 100 scaling (n1-standard-4 instances)
â”‚  â”œâ”€ Base: 50 Ã— $60/month = $3,000
â”‚  â”œâ”€ Scaling (avg 50% of time): 50 Ã— $60 Ã— 0.5 = $1,500
â”‚  â””â”€ Total compute: $4,500
â”‚
â”œâ”€ Load Balancer & API Gateway
â”‚  â””â”€ Cost: $150 (Google Cloud Load Balancer)
â”‚
â””â”€ Total Compute: $4,950

DATABASE:
â”œâ”€ PostgreSQL Primary (3-node high-availability cluster)
â”‚  â””â”€ 32 GB RAM, SSD: $2,000
â”‚
â”œâ”€ PostgreSQL Replica (read-only, for analytics)
â”‚  â””â”€ 16 GB RAM, SSD: $1,000
â”‚
â”œâ”€ Redis Cache (3GB)
â”‚  â””â”€ $500 (Cloud Memorystore)
â”‚
â”œâ”€ Vector DB (Supabase pgvector, included in Postgres)
â”‚  â””â”€ $0 (included)
â”‚
â”œâ”€ Cloud Storage (PDFs, generated documents)
â”‚  â””â”€ 1TB active storage: $20/month
â”‚
â””â”€ Total Database: $3,520

LLM INFERENCE:
â”œâ”€ OpenAI API: 115 req/sec Ã— 86,400 sec/day Ã— 30 days
â”‚  = 297,792,000 requests/month
â”‚  â”œâ”€ Assume 60% use gpt-3.5 (cheap): 200k tokens
â”‚    Cost: 0.0005 Ã— (200k/1k) Ã— 60% of requests = $36,000
â”‚  â”œâ”€ Assume 30% use gpt-4 (moderate): 500k tokens
â”‚    Cost: 0.015 Ã— (500k/1k) Ã— 30% of requests = $67,500
â”‚  â”œâ”€ Assume 10% use gpt-5 (high quality): 800k tokens
â”‚    Cost: 0.05 Ã— (800k/1k) Ã— 10% of requests = $40,000
â”‚  â””â”€ OpenAI total: ~$143,500
â”‚
â”œâ”€ Google Gemini: 115 req/sec Ã— 30 days (fallback)
â”‚  â””â”€ Cost: ~$45,000 (fallback usage, lower cost)
â”‚
â”œâ”€ Anthropic Claude: 115 req/sec Ã— 30 days (specialty)
â”‚  â””â”€ Cost: ~$60,000
â”‚
â””â”€ Total LLM: ~$248,500

NETWORKING:
â”œâ”€ Data transfer (outbound): 100GB/month @ $0.12/GB = $12,000
â”œâ”€ Cross-region replication: ~$5,000
â””â”€ Total Networking: $17,000

MONITORING & LOGGING:
â”œâ”€ Cloud Logging: $5,000 (1TB/month logs)
â”œâ”€ Cloud Monitoring: $1,000
â”œâ”€ Error Tracking: $500
â””â”€ Total Monitoring: $6,500

SECURITY & COMPLIANCE:
â”œâ”€ VPN/Private network: $1,000
â”œâ”€ Backup storage (3 copies, 7 days retention): $2,000
â”œâ”€ Security scanning: $500
â””â”€ Total Security: $3,500

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
TOTAL MONTHLY COST: ~$283,970

Cost per user per month: $283,970 Ã· 1,000,000 = $0.28
Cost per request: $283,970 Ã· (115 req/sec Ã— 86,400 Ã— 30) = $0.000095
```

### 8.2 Cost Optimization Strategies

```
Strategy 1: Smart Model Routing (Oracle Framework)
â”œâ”€ Current: Assume even distribution across models
â”œâ”€ Optimized: 70% cheap models, 20% moderate, 10% premium
â””â”€ Savings: ~40% LLM costs = $99,400/month saved

Strategy 2: Request Caching
â”œâ”€ Current: All requests hit LLM
â”œâ”€ Optimized: Cache 35% of identical requests (same resume + same job desc)
â”œâ”€ Savings: 35% Ã— $248,500 = $86,975/month saved

Strategy 3: Spot Instance Workers
â”œâ”€ Current: On-demand instances @ $0.095/hour
â”œâ”€ Optimized: Spot instances @ $0.028/hour (70% discount, Google guarantee)
â”œâ”€ Compute savings: 70% Ã— $4,500 = $3,150/month

Strategy 4: Batch Processing
â”œâ”€ Current: Process each request individually
â”œâ”€ Optimized: Batch similar requests (e.g., 20 resumes â†’ 1 LLM call)
â”œâ”€ Challenge: Need to aggregate results accurately
â”œâ”€ Savings: 20% of LLM calls = $49,700/month (if feasible)

Strategy 5: Regional Optimization
â”œâ”€ Current: Same pricing globally
â”œâ”€ Optimized: Route low-priority tasks to cheaper regions (compute arbitrage)
â”œâ”€ Potential savings: 15-20% compute ($750/month)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Total Monthly Savings with all strategies: ~$240K
New monthly cost: ~$44K
New cost per user: $0.044/month ($0.53/user/year)
New cost per request: $0.000015
```

---

## 9. SECURITY & ISOLATION LAYER

### 9.1 Multi-Tenancy Isolation

With 100 independent Spoke apps running on one CDM, isolation is critical.

```
NETWORK ISOLATION:
â”œâ”€ Each tenant gets isolated namespace in Temporal
â”‚  â””â”€ Workflow namespace: "achievemore_resume_optimizer"
â”‚  â””â”€ Workflow namespace: "achievemore_legal_analyzer"
â”‚  â””â”€ Workflow namespace: "achievemore_medical_checker"
â”‚
â”œâ”€ Database Row-Level Security (RLS)
â”‚  â””â”€ SELECT user_resume WHERE user_id = ${authenticated_user_id}
â”‚  â””â”€ PostgreSQL enforces at database level
â”‚
â”œâ”€ Redis Key Prefixing
â”‚  â””â”€ User data: "user:{user_id}:*"
â”‚  â””â”€ App data: "app:{app_id}:*"
â”‚  â””â”€ No cross-access possible
â”‚
â””â”€ Vector DB Isolation
   â””â”€ Each app has separate vector index
   â””â”€ Query returns only within app partition

COMPUTE ISOLATION:
â”œâ”€ Container sandboxing
â”‚  â””â”€ Worker runs in isolated Docker container
â”‚  â””â”€ --memory 512m, --cpus 1 (resource limits)
â”‚  â””â”€ --read-only filesystem (except /tmp)
â”‚
â”œâ”€ Agent execution limits
â”‚  â”œâ”€ Timeout: 5 minutes max per agent run
â”‚  â”œâ”€ Memory: 512MB max per agent
â”‚  â”œâ”€ Network: Only whitelisted external APIs allowed
â”‚  â””â”€ File access: Only /tmp and object storage
â”‚
â””â”€ API isolation
   â””â”€ Each spoke has unique API key
   â””â”€ Rate limiting: 100 req/sec per app
   â””â”€ Quota: 1M requests/month per app (paid tier)

DATA ISOLATION:
â”œâ”€ At-rest encryption
â”‚  â””â”€ All Postgres: AES-256 encryption
â”‚  â””â”€ All object storage: Google-managed keys
â”‚  â””â”€ Redis: TLS in-transit
â”‚
â”œâ”€ In-transit encryption
â”‚  â””â”€ All APIs: HTTPS/TLS 1.3
â”‚  â””â”€ Database connections: SSL required
â”‚  â””â”€ Replication: Encrypted tunnels
â”‚
â””â”€ Encryption key management
   â””â”€ Keys stored in Google Cloud KMS
   â””â”€ Automatic key rotation (monthly)
   â””â”€ Access logs for all key operations
```

---

## 10. MONITORING, LOGGING & OBSERVABILITY

### 10.1 Three-Pillar Observability

```
PILLAR 1: LOGS (What happened?)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Google Cloud Logging                     â”‚
â”‚ â”œâ”€ All agent actions logged              â”‚
â”‚ â”œâ”€ All LLM calls logged (input + output) â”‚
â”‚ â”œâ”€ All database operations logged        â”‚
â”‚ â”œâ”€ Retention: 30 days (searchable)       â”‚
â”‚ â””â”€ Archive: 7 years (Cloud Storage)      â”‚
â”‚                                          â”‚
â”‚ Example log entry:                       â”‚
â”‚ {                                        â”‚
â”‚   "timestamp": "2026-01-05T14:32:10Z",   â”‚
â”‚   "workflow_id": "wf_abc123",            â”‚
â”‚   "activity": "call_llm_optimized",      â”‚
â”‚   "model": "gpt-4-turbo",                â”‚
â”‚   "input_tokens": 450,                   â”‚
â”‚   "output_tokens": 320,                  â”‚
â”‚   "latency_ms": 1850,                    â”‚
â”‚   "status": "success",                   â”‚
â”‚   "cost": 0.0089,                        â”‚
â”‚   "user_id": "user_xyz"                  â”‚
â”‚ }                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PILLAR 2: METRICS (What's the trend?)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Google Cloud Monitoring + Prometheus      â”‚
â”‚                                          â”‚
â”‚ System Metrics (1-minute granularity):   â”‚
â”‚ â”œâ”€ CPU usage: 45% (alert if >80%)       â”‚
â”‚ â”œâ”€ Memory: 2.1GB / 4GB (alert if >85%)  â”‚
â”‚ â”œâ”€ Disk: 450GB / 500GB (alert if >90%)  â”‚
â”‚ â”œâ”€ Network: 120 Mbps in, 80 Mbps out    â”‚
â”‚ â””â”€ Requests/sec: 115 (expected)         â”‚
â”‚                                          â”‚
â”‚ Application Metrics:                     â”‚
â”‚ â”œâ”€ Workflow success rate: 99.2%         â”‚
â”‚ â”œâ”€ Workflow p50 latency: 2.1s           â”‚
â”‚ â”œâ”€ Workflow p99 latency: 8.3s           â”‚
â”‚ â”œâ”€ LLM success rate: 97.8%              â”‚
â”‚ â”œâ”€ Cache hit rate: 38%                  â”‚
â”‚ â”œâ”€ Average cost per request: $0.00015   â”‚
â”‚ â””â”€ Active user sessions: 45,000         â”‚
â”‚                                          â”‚
â”‚ Business Metrics:                        â”‚
â”‚ â”œâ”€ Requests completed: 100M/month       â”‚
â”‚ â”œâ”€ Revenue: $50K/month                  â”‚
â”‚ â”œâ”€ Monthly recurring revenue: $140K     â”‚
â”‚ â”œâ”€ Churn rate: 2.1%                    â”‚
â”‚ â””â”€ Customer satisfaction: 4.7/5.0       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PILLAR 3: TRACES (How did it flow?)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Google Cloud Trace + Jaeger               â”‚
â”‚                                          â”‚
â”‚ Distributed trace for single request:    â”‚
â”‚ request_id: req_20260105_abc123          â”‚
â”‚                                          â”‚
â”‚ â”œâ”€ [0ms] API Gateway                    â”‚
â”‚ â”‚  â””â”€ Auth check: 5ms                    â”‚
â”‚ â”‚  â””â”€ Rate limit check: 2ms              â”‚
â”‚ â”œâ”€ [7ms] Temporal Workflow Created       â”‚
â”‚ â”œâ”€ [10ms] Activity: download_resume      â”‚
â”‚ â”‚  â””â”€ Fetch from resume_url: 45ms       â”‚
â”‚ â”‚  â””â”€ Validate PDF: 8ms                  â”‚
â”‚ â”œâ”€ [63ms] Activity: extract_content      â”‚
â”‚ â”‚  â””â”€ OCR (if needed): 120ms            â”‚
â”‚ â”‚  â””â”€ Text extraction: 15ms              â”‚
â”‚ â”œâ”€ [198ms] Activity: oracle_decide       â”‚
â”‚ â”‚  â””â”€ Cache hit! (return cached): 3ms   â”‚
â”‚ â”œâ”€ [201ms] Activity: call_llm_optimized â”‚
â”‚ â”‚  â””â”€ Prompt building: 10ms              â”‚
â”‚ â”‚  â””â”€ LLM call (gpt-4-turbo): 1850ms    â”‚
â”‚ â”‚  â””â”€ Response parsing: 5ms              â”‚
â”‚ â”œâ”€ [2066ms] Activity: calculate_ats     â”‚
â”‚ â”‚  â””â”€ Scoring logic: 250ms               â”‚
â”‚ â”œâ”€ [2316ms] Activity: generate_document â”‚
â”‚ â”‚  â””â”€ DOCX generation: 800ms            â”‚
â”‚ â”‚  â””â”€ Upload to storage: 450ms          â”‚
â”‚ â”œâ”€ [3566ms] Activity: store_result      â”‚
â”‚ â”‚  â””â”€ Database write: 15ms               â”‚
â”‚ â”‚  â””â”€ Webhook call: 50ms                 â”‚
â”‚ â””â”€ [3631ms] COMPLETE (total: 3.6 sec)   â”‚
â”‚                                          â”‚
â”‚ Cost attribution:                        â”‚
â”‚ â”œâ”€ LLM cost: $0.0089 (gpt-4 tokens)    â”‚
â”‚ â”œâ”€ Compute cost: $0.0004                â”‚
â”‚ â”œâ”€ Storage cost: $0.00002               â”‚
â”‚ â””â”€ Total: $0.00932                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. DISASTER RECOVERY & HIGH AVAILABILITY

### 11.1 RTO/RPO Targets

```
RECOVERY OBJECTIVES:
â”œâ”€ RTO (Recovery Time Objective): 5 minutes
â”‚  â””â”€ If primary region down, failover to backup in <5 min
â”‚
â”œâ”€ RPO (Recovery Point Objective): 1 minute
â”‚  â””â”€ If data loss, max 1 minute of data lost
â”‚
â””â”€ Availability Target: 99.95% (52 minutes downtime/year)

MULTI-REGION ACTIVE-ACTIVE:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ US-EAST1 (Active)          EU-WEST1 (Active)            â”‚
â”‚ â”œâ”€ Temporal Primary         â”œâ”€ Temporal Replica          â”‚
â”‚ â”œâ”€ Postgres Primary         â”œâ”€ Postgres Replica          â”‚
â”‚ â”œâ”€ Redis Leader             â”œâ”€ Redis Replica             â”‚
â”‚ â”œâ”€ 50 active workers        â”œâ”€ 30 active workers        â”‚
â”‚ â””â”€ Serves 60% traffic       â””â”€ Serves 40% traffic       â”‚
â”‚                                                         â”‚
â”‚         â†• Continuous Replication                        â”‚
â”‚         â€¢ Postgres logical replication (<500ms lag)     â”‚
â”‚         â€¢ Redis active-active replication                â”‚
â”‚         â€¢ Temporal workflow history replicated           â”‚
â”‚                                                         â”‚
â”‚ If US-EAST1 fails:                                      â”‚
â”‚ â”œâ”€ Global LB detects no health check response           â”‚
â”‚ â”œâ”€ Switches 100% traffic to EU-WEST1                    â”‚
â”‚ â”œâ”€ EU-WEST1 scales up workers 30â†’100 (auto-scale)      â”‚
â”‚ â”œâ”€ Postgres EU reads become primary (failover)          â”‚
â”‚ â””â”€ Time to full service: ~2 minutes                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BACKUP STRATEGY:
â”œâ”€ Continuous backup (every change)
â”‚  â”œâ”€ Postgres: Binary log shipping to Cloud Storage      â”‚
â”‚  â”œâ”€ Object storage: Versioning enabled                  â”‚
â”‚  â””â”€ Retention: 30 days daily + 365 days monthly         â”‚
â”‚
â”œâ”€ Point-in-time recovery
â”‚  â””â”€ Restore database to any second within 7 days        â”‚
â”‚
â””â”€ Backup testing
   â””â”€ Monthly: Restore backup to staging, verify          â”‚
```

---

## 12. PRODUCTION DEPLOYMENT CHECKLIST

### 12.1 Pre-Deployment

- [ ] **Code Review**
  - [ ] Peer reviewed all changes
  - [ ] No hardcoded API keys or passwords
  - [ ] No console.log or debug statements
  - [ ] Type safety checks passed (TypeScript compilation)

- [ ] **Security Scan**
  - [ ] Snyk scan for vulnerability
  - [ ] No SQL injection possible
  - [ ] Authentication enforced on all endpoints
  - [ ] Rate limiting configured

- [ ] **Performance Testing**
  - [ ] Load test: 1000 concurrent users
  - [ ] Stress test: 5000 concurrent users
  - [ ] Latency p99 < 10 seconds
  - [ ] Error rate < 0.1%

- [ ] **Database**
  - [ ] Migrations tested on staging
  - [ ] Rollback scripts prepared
  - [ ] Backups verified
  - [ ] Indexes analyzed

- [ ] **Configuration**
  - [ ] Environment variables set in KMS
  - [ ] API keys rotated (if needed)
  - [ ] Monitoring alerts configured
  - [ ] Log retention policy set

### 12.2 Deployment Execution

```bash
#!/bin/bash
# /scripts/deploy-production.sh

set -e

# Color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo -e "${YELLOW}ğŸš€ Starting production deployment...${NC}"

# Step 1: Blue-Green Preparation
echo "Step 1/8: Preparing blue-green deployment..."
gcloud run deploy ii-agent-green \
    --image gcr.io/$GCP_PROJECT_ID/ii-agent:${VERSION} \
    --no-traffic \
    --memory 2Gi \
    --region us-central1

# Smoke tests on green
echo "Running smoke tests on green instance..."
GREEN_URL=$(gcloud run describe ii-agent-green --region us-central1 --format 'value(status.url)')
if ! curl -s ${GREEN_URL}/health; then
    echo -e "${RED}âœ— Smoke tests failed${NC}"
    exit 1
fi
echo -e "${GREEN}âœ“ Smoke tests passed${NC}"

# Step 2: Database Migration
echo "Step 2/8: Running database migrations..."
cd /opt/achievemore/CommonGround
npm run db:migrate

# Step 3: Traffic Shift (Canary)
echo "Step 3/8: Starting canary deployment (5% traffic)..."
gcloud run services update-traffic ii-agent \
    --to-revisions ii-agent-green=5,ii-agent=95 \
    --region us-central1

sleep 60  # Monitor for 1 minute

# Check error rate
ERROR_RATE=$(gcloud monitoring time-series list \
    --filter 'metric.type=run.googleapis.com/request_count AND resource.service_name=ii-agent' \
    --interval-end-time 2026-01-05T14:33:00Z \
    --interval-start-time 2026-01-05T14:32:00Z | grep error_count || echo "0")

if [ "$ERROR_RATE" -gt "10" ]; then
    echo -e "${RED}âœ— Error rate too high, rolling back${NC}"
    gcloud run services update-traffic ii-agent \
        --to-revisions ii-agent=100 \
        --region us-central1
    exit 1
fi

# Step 4: Increase Canary
echo "Step 4/8: Increasing canary to 25%..."
gcloud run services update-traffic ii-agent \
    --to-revisions ii-agent-green=25,ii-agent=75 \
    --region us-central1

sleep 120  # Monitor for 2 minutes

# Step 5: Full Traffic Shift
echo "Step 5/8: Shifting 100% traffic to green..."
gcloud run services update-traffic ii-agent \
    --to-revisions ii-agent-green=100 \
    --region us-central1

# Step 6: Promote Green to Blue
echo "Step 6/8: Promoting green to primary..."
gcloud run services update ii-agent \
    --traffic ii-agent-green=100 \
    --region us-central1

# Rename for clarity
gcloud run deploy ii-agent-blue \
    --image gcr.io/$GCP_PROJECT_ID/ii-agent:${VERSION} \
    --region us-central1

# Step 7: Cleanup Old Blue
echo "Step 7/8: Cleaning up old revision..."
# Keep last 3 revisions for rollback
gcloud run revisions list --service ii-agent --region us-central1 \
    --sort-by='~created' --limit=3 | tail -n +4 | while read revision; do
    gcloud run revisions delete $revision --region us-central1 --quiet
done

# Step 8: Post-Deployment Verification
echo "Step 8/8: Running post-deployment checks..."

echo "Checking application health..."
HEALTH_URL=$(gcloud run describe ii-agent --region us-central1 --format 'value(status.url)')
for i in {1..10}; do
    if curl -s ${HEALTH_URL}/health > /dev/null; then
        echo -e "${GREEN}âœ“ Health check passed${NC}"
        break
    fi
    sleep 2
done

echo "Verifying database..."
npm run db:verify

echo "Running integration tests..."
npm run test:integration

echo ""
echo -e "${GREEN}âœ… Production deployment complete!${NC}"
echo "Deployed version: ${VERSION}"
echo "Service URL: ${HEALTH_URL}"
```

---

## CONCLUSION & NEXT STEPS

This document provides the complete blueprint for operating AchieveMore at 1M concurrent users.

### Key Takeaways:

1. **Central Development Machine (CDM)** is the heart - it orchestrates 100+ Spokes
2. **Oracle Framework** routes requests intelligently (cost-efficient)
3. **Temporal** handles all the hard distributed system problems
4. **Multi-region sharding** enables global scale
5. **CLI toolkit** manages everything via terminal

### Immediate Implementation (Week 1):

```bash
# Install CDM
./scripts/install-cdm.sh

# Deploy first Spoke (Resume Optimizer)
achievemore deploy production

# Monitor
achievemore status
achievemore metrics

# Scale as needed
achievemore scale 100  # 100 workers for 10K concurrent users
```

### Success Metrics to Track:

```yaml
Daily:
  - Workflow success rate: >99.5%
  - Average latency: <3 seconds
  - Error rate: <0.1%

Weekly:
  - Cost per request: <$0.001
  - Cache hit rate: >30%
  - Uptime: >99.95%

Monthly:
  - Revenue per request: $0.005+
  - Gross margin: >70%
  - Churn rate: <2%
```

---

**Document prepared by:** Platform Engineering Team  
**Last updated:** January 5, 2026  
**Next review:** March 5, 2026

For updates and support: https://docs.achievemore.com
